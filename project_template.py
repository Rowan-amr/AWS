# -*- coding: utf-8 -*-
"""project-template.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_bFUOHP10L8S9ueUmIFQ2DpTsPLGXhzK

# Predict Bike Sharing Demand with AutoGluon Template

## Project: Predict Bike Sharing Demand with AutoGluon
This notebook is a template with each step that you need to complete for the project.

Please fill in your code where there are explicit `?` markers in the notebook. You are welcome to add more cells and code as you see fit.

Once you have completed all the code implementations, please export your notebook as a HTML file so the reviews can view your code. Make sure you have all outputs correctly outputted.

`File-> Export Notebook As... -> Export Notebook as HTML`

There is a writeup to complete as well after all code implememtation is done. Please answer all questions and attach the necessary tables and charts. You can complete the writeup in either markdown or PDF.

Completing the code template and writeup template will cover all of the rubric points for this project.

The rubric contains "Stand Out Suggestions" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the "stand out suggestions", you can include the code in this notebook and also discuss the results in the writeup file.

## Step 1: Create an account with Kaggle

### Create Kaggle Account and download API key
Below is example of steps to get the API username and key. Each student will have their own username and key.

1. Open account settings.
![kaggle1.png](attachment:kaggle1.png)
![kaggle2.png](attachment:kaggle2.png)
2. Scroll down to API and click Create New API Token.
![kaggle3.png](attachment:kaggle3.png)
![kaggle4.png](attachment:kaggle4.png)
3. Open up `kaggle.json` and use the username and key.
![kaggle5.png](attachment:kaggle5.png)

## Step 2: Download the Kaggle dataset using the kaggle python library

### Open up Sagemaker Studio and use starter template

1. Notebook should be using a `ml.t3.medium` instance (2 vCPU + 4 GiB)
2. Notebook should be using kernal: `Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)`

### Install packages
"""

!pip install -U pip
!pip install -U setuptools wheel
!pip install -U "mxnet<2.0.0" bokeh==2.0.1
!pip install autogluon --no-cache-dir
# Without --no-cache-dir, smaller aws instances may have trouble installing

"""### Setup Kaggle API Key"""

# create the .kaggle directory and an empty kaggle.json file
!mkdir -p /root/.kaggle
!touch /root/.kaggle/kaggle.json
!chmod 600 /root/.kaggle/kaggle.json

# Fill in your user name and key from creating the kaggle account and API token file
import json
kaggle_username = "rowanamr"
kaggle_key = "714dc92412ff3f072b8bd00ed314fb17"

# Save API token the kaggle.json file
with open("/root/.kaggle/kaggle.json", "w") as f:
    f.write(json.dumps({"username": kaggle_username, "key": kaggle_key}))

"""### Download and explore dataset

### Go to the bike sharing demand competition and agree to the terms
![kaggle6.png](attachment:kaggle6.png)
"""

# Download the dataset, it will be in a .zip file so you'll need to unzip it as well.
!kaggle competitions download -c bike-sharing-demand
# If you already downloaded it you can use the -o command to overwrite the file
!unzip -o bike-sharing-demand.zip

pip install autogluon

import pandas as pd
from autogluon.tabular import TabularPredictor

from sklearn import datasets
import numpy as np
from datetime import datetime
import seaborn as sb
import matplotlib.pyplot as plt

# Create the train dataset in pandas by reading the csv
# Set the parsing of the datetime column so you can use some of the `dt` features in pandas later
train = pd.read_csv('train.csv', parse_dates=['datetime'])
train.head()

from google.colab import drive
drive.mount('/content/drive')

# Simple output of the train dataset to view some of the min/max/varition of the dataset features.
train.describe()

#identifying the data types of the training data
train.info()

# Create the test pandas dataframe in pandas by reading the csv, remember to parse the datetime!
test = pd.read_csv('test.csv', parse_dates = ['datetime'])
test.head()

# Same thing as train and test dataset
submission = pd.read_csv('sampleSubmission.csv', parse_dates=['datetime'])
submission.head()

submission.describe()

print('Train Columns:', train.columns,"\nTest Columns:", test.columns)

"""## Step 3: Train a model using AutoGluonâ€™s Tabular Prediction

Requirements:
* We are prediting `count`, so it is the label we are setting.
* Ignore `casual` and `registered` columns as they are also not present in the test dataset.
* Use the `root_mean_squared_error` as the metric to use for evaluation.
* Set a time limit of 10 minutes (600 seconds).
* Use the preset `best_quality` to focus on creating the best model.
"""

predictor = TabularPredictor(label = 'count', eval_metric='root_mean_squared_error',
                             learner_kwargs={'ignored_columns':['casual', 'registered']}).fit(train_data = train, time_limit = 600,presets = 'best_quality')

"""### Review AutoGluon's training run with ranking of models that did the best."""

predictor.fit_summary()

"""### Create predictions from test dataset"""

#Extracting leaderboard of models as a dataframe
leaderboard = pd.DataFrame(predictor.leaderboard(silent=True))
leaderboard

sb.barplot(data = leaderboard, x ='model', y = 'score_val', palette = 'viridis');
plt.xticks(rotation = 90);

predictions = predictor.predict(test)
predictions.head()

"""#### NOTE: Kaggle will reject the submission if we don't set everything to be > 0."""

# Describe the `predictions` series to see if there are any negative values
predictions.describe()

# How many negative values do we have?
predictions[predictions<0].count()

# Set them to zero: NO need cause  we have no negative values

"""### Set predictions to submission dataframe, save, and submit"""

submission["count"] = pd.DataFrame(predictions)
submission.to_csv("submission.csv", index=False)
submission.head()

!kaggle competitions submit -c bike-sharing-demand -f submission.csv -m "first raw submission"

"""#### View submission via the command line or in the web browser under the competition's page - `My Submissions`"""

!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6

"""#### Initial score of 1.80464

## Step 4: Exploratory Data Analysis and Creating an additional feature
* Any additional feature will do, but a great suggestion would be to separate out the datetime into hour, day, or month parts.
"""

# Create a histogram of all features to show the distribution of each one relative to the data. This is part of the exploritory data analysis
train.hist(figsize = (12,10), color = 'purple');
plt.tight_layout();
plt.xticks(fontsize=10);

# create a new feature
train['month'] = train['datetime'].dt.month
train['hour'] = train['datetime'].dt.hour
train['day'] = train['datetime'].dt.day
test['month'] = test['datetime'].dt.month
test['hour'] = test['datetime'].dt.hour
test['day'] = test['datetime'] .dt.day

#differentiating the count per hour for the bike
sb.catplot(data = train, x = 'hour', y = 'count',kind = 'bar', palette = 'viridis')

"""## Make category types for these so models know they are not just numbers
* AutoGluon originally sees these as ints, but in reality they are int representations of a category.
* Setting the dtype to category will classify these as categories in AutoGluon.
"""

train["season"] = train['season'].astype('category')
train["weather"] = train['weather'].astype('category')
test["season"] = test['season'].astype('category')
test["weather"] = test['weather'].astype('category')

# View are new feature
train.head()

# View histogram of all features again now with the hour feature
train.hist(figsize = (15,20),color = 'purple');
plt.tight_layout()

"""
## Step 5: Rerun the model with the same settings as before, just with more features"""

#Using XGBOOST as the algorithm
predictor_new_features = TabularPredictor(label = 'count', eval_metric = 'root_mean_squared_error',
                                          learner_kwargs = {'ignored_columns':['casual', 'registered']}).fit(train_data = train, time_limit = 600, presets = 'best_quality',hyperparameters = {'XGB':{'learning_rate':0.1}})

predictor_new_features.fit_summary()

leaderboard_new = pd.DataFrame(predictor_new_features.leaderboard(silent=True))

plt.xticks(rotation = 90);
sb.barplot(data = leaderboard_new, x = 'model', y = 'score_val', palette = 'viridis');

predictions_new_features = predictor_new_features.predict(test)
predictions_new_features.head()

predictions_new_features.describe()

# Remember to set all negative values to zero
predictions_new_features [predictions_new_features < 0].count()

#Setting all -ve values to zero
for a,b in enumerate(predictions_new_features):
    if b < 0:
        predictions_new_features[a] = 0

predictions_new_features [predictions_new_features < 0].count()

# extracting column names
train.columns

# Same submitting predictions
submission_new_features['count']= predictions_new_features
submission_new_features.to_csv("submission_new_features.csv", index=False)
submission_new_features.head()

!kaggle competitions submit -c bike-sharing-demand -f submission_new_features.csv -m "new features"

!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6

"""#### New Score of 1.80464

## Step 6: Hyper parameter optimization
* There are many options for hyper parameter optimization.
* Options are to change the AutoGluon higher level parameters or the individual model hyperparameters.
* The hyperparameters of the models themselves that are in AutoGluon. Those need the `hyperparameter` and `hyperparameter_tune_kwargs` arguments.
"""

predictor_new_hpo = TabularPredictor(label = 'count', eval_metric='root_mean_squared_error',
                                     learner_kwargs = {'ignored_columns': ['casual','registered']}).fit(train_data = train,
                                                                                                        presets = 'optimize_for_deployment',hyperparameters = {'GBM':[{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}},'GBMLarge']}, refit_full='best',
                                                                                                        hyperparameter_tune_kwargs={'num_trials': 100,
                                                                                                        'search_strategy': 'random','scheduler':'local'})

predictor_new_hpo.fit_summary()

# Leaderboard dataframe
leaderboard_new_hpo = pd.DataFrame(predictor_new_hpo.leaderboard(silent=True))
leaderboard_new_hpo

sb.barplot(data = leaderboard_new_hpo, x ='model', y = 'score_val', palette = 'viridis');
plt.xticks(rotation = 90);

predictions_new_hpo = predictor_new_hpo.predict(test)
predictions_new_hpo.head()

predictions_new_hpo.describe()

predictions_new_hpo[predictions_new_hpo<0].count()

# Remember to set all negative values to zero
for a,b in enumerate(predictions_new_hpo):
    if b < 0:
        predictions_new_hpo[a] = 0

predictions_new_hpo[predictions_new_hpo<0].count()

# Same submitting predictions
submission_new_hpo['count'] = predictions_new_hpo
submission_new_hpo.to_csv("submission_new_hpo.csv", index=False)
submission_new_hpo.head()

!kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo.csv -m "new features with hyperparameters"

!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6

"""#### New Score of `?`

## Step 7: Write a Report
### Refer to the markdown file for the full report
### Creating plots and table for report
"""

# Taking the top model score from each training run and creating a line plot to show improvement
# You can create these in the notebook and save them to PNG or use some other tool (e.g. google sheets, excel)
fig = pd.DataFrame(
    {
        "model": ["initial", "add_features", "hpo"],
        "score": [?, ?, ?]
    }
).plot(x="model", y="score", figsize=(8, 6)).get_figure()
fig.savefig('model_train_score.png')

# Take the 3 kaggle scores and creating a line plot to show improvement
fig = pd.DataFrame(
    {
        "test_eval": ["initial", "add_features", "hpo"],
        "score": [?, ?, ?]
    }
).plot(x="test_eval", y="score", figsize=(8, 6)).get_figure()
fig.savefig('model_test_score.png')

"""### Hyperparameter table"""

# The 3 hyperparameters we tuned with the kaggle score as the result
pd.DataFrame({
    "model": ["initial", "add_features", "hpo"],
    "hpo1": [?, ?, ?],
    "hpo2": [?, ?, ?],
    "hpo3": [?, ?, ?],
    "score": [?, ?, ?]
})